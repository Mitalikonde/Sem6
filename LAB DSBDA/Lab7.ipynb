{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc077c5-4be4-4f4e-be34-b55d0927cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mitali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mitali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mitali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "#Note :  pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0dc5ab-fa4a-466e-a942-72c4458fe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(document)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Stop words removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "def calculate_tf_idf(documents):\n",
    "    # Join preprocessed tokens into documents for TF-IDF calculation\n",
    "    preprocessed_documents = [' '.join(doc) for doc in documents]\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "    \n",
    "    # Get feature names (terms)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_matrix, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b11e62-a4ee-49f8-8190-3a3802250430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      "   analysis  artificial   assigns      base    common  computer  concerned  \\\n",
      "0  0.108465    0.108465  0.108465  0.108465  0.108465  0.108465   0.108465   \n",
      "\n",
      "      field  filtered      form  ...   similar    speech  splitting  stemming  \\\n",
      "0  0.108465  0.108465  0.108465  ...  0.108465  0.108465   0.108465   0.21693   \n",
      "\n",
      "       stop   tagging     text  tokenization     valid      word  \n",
      "0  0.108465  0.108465  0.21693      0.108465  0.108465  0.650791  \n",
      "\n",
      "[1 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sample document\n",
    "sample_document = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence \n",
    "concerned with the interaction between computers and humans in natural \n",
    "language. Tokenization is the process of splitting a text into words and \n",
    "punctuation marks. POS tagging assigns parts of speech to each word in a \n",
    "sentence. Stop words are common words that are often filtered out in text \n",
    "analysis. Stemming reduces words to their root or base form. Lemmatization \n",
    "is similar to stemming but produces valid words.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the sample document\n",
    "preprocessed_tokens = preprocess_document(sample_document)\n",
    "\n",
    "# Calculate TF-IDF representation\n",
    "tfidf_matrix, feature_names = calculate_tf_idf([preprocessed_tokens])\n",
    "\n",
    "# Display TF-IDF representation\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101200c4-aafe-4553-bd4b-1c80189794c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
